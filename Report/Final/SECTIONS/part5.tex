\documentclass[../main.tex]{subfiles}
\input{main}

\begin{document}

\subsection{Purpose and range of the simulator}

The simulator is designed to test various algorithms and methods for multi-robot exploration of caves. The robots operate in the air while moving along the floor. To simplify the problem, we approximate the floor as a 2D plane without any surface irregularities.

\subsection{Creation of the map}

The map consist in succession of straight lines generate by a simple cellular automata. 

A cellular automaton is a grid of cells, where each cell can exist in different states based on predefined rules. The concept was developed by Stanislaw Ulam and John von Neumann in the 1940s.  

The most famous cellular automaton, which helped popularize its use, was developed by John Conway: \textit{Conway's Game of Life}. This model follows a set of four simple rules, which can be found \href{URL}{here}. The rules are based on the properties of neighboring cells.  

There are two commonly used neighborhood types in cellular automata:  
\begin{itemize}  
    \item \textbf{Von Neumann neighborhood}, which considers the four direct neighbors (left, top, right and bottom).  
    \item \textbf{Moore neighborhood}, an extension that includes all eight surrounding cells, both diagonal and direct neighbors.  
\end{itemize}


The beauty of this system lies in the complexity that emerges from such simple rules. Beginning with the configuration shown in \autoref{fig:startconfigcgof}, the system evolves into the states illustrated in \autoref{fig:CGOF} at generations 87 and 263.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{IMAGES/part5/StartConfigCGOF.png}
        \caption{Iteration 0}
		\label{fig:startconfigcgof}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{IMAGES/part5/CGOF_1.png}
        \caption{Generation 87}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{IMAGES/part5/CGOF_2.png}
        \caption{Generation 263}
    \end{subfigure}
    \caption{Conway's Game of Life}
    \label{fig:CGOF}
\end{figure}

Programmers, computer scientists, and mathematicians began cataloging all the \href{https://en.wikipedia.org/wiki/Conway's_Game_of_Life#Examples_of_patterns}{patterns they encountered} and constructed remarkably complex machines.

In our case, the rules are defined as follows:
\begin{itemize}  
    \item If there are more than 4 activated cells in the Moore neighborhood, the cell activates. 
    \item Otherwise, the cell deactivates. 
\end{itemize}

I implement the method on a Cartesian grid and then transform it into a triangular mesh, as shown in \autoref{fig:triag_transform}. The maps generated after this transformation improved in quality.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{IMAGES/part5/grid_transform_map_generation.png}
	\caption{Grid transformation}
	\label{fig:triag_transform}
\end{figure}

Using the Marching Squares method, I draw the boundaries between occupied cells (red dots) and unoccupied cells (green dots).

Marching Squares is a technique for generating the contours of a two-dimensional grid, which, in our case, is a triangular mesh. As we traverse the domain, the boundaries are drawn accordingly. \autoref{fig:marching_triag} illustrates all possible states of an element composed of three cells.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{IMAGES/part5/marching_square_triangle.png}
	\caption{Isolines, possible states of a triangular element}
	\label{fig:marching_triag}
\end{figure}

At the end of both steps—transformation and Marching Squares—we obtain the following schematic representation:

\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\linewidth]{IMAGES/part5/map_generation_example.png}
	\caption{Exemple scheme of map generated}
	\label{fig:map_gen_scheme}
\end{figure}

In the simulator, maps are generated based on a seed and a random generator. The shape of each map is controlled by three parameters: the step size along the x-axis, the step size along the y-axis, and the overall map size.

This allows for a vast variety of map configurations. \autoref{fig:three_map_example} illustrate some examples.
\begin{figure}[H]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{IMAGES/part5/map_dx100.png}
        \caption{$\delta x = 100$ mu}
		\label{fig:real_map_100}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{IMAGES/part5/map_dx40.png}
        \caption{$\delta x = 40$ mu}
		\label{fig:real_map_40}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{IMAGES/part5/map_dx10.png}
        \caption{$\delta x = 10$ mu}
		\label{fig:real_map_10}
    \end{subfigure}
    \caption{Different maps made with equilateral triangles}
    \label{fig:three_map_example}
\end{figure}

\tobedone


\subsection{Robot implementation}

\subsubsection{Robot model}

As a robot, I used a simple diferential drive robot with two idle-wheels and 2 driven wheels.

A model of the robot can be found on \autoref{fig:robot_model} and \autoref{fig:robot_3Dmodel}.

\begin{figure}[H]
	\centering
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{IMAGES/part5/robot_model_2.png}
		\caption{Differential drive robot diagram}
		\label{fig:robot_model}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{IMAGES/part5/robot_3Dmodel.png}
		\caption{Differential drive robot 3D model}
		\label{fig:robot_3Dmodel}
	\end{subfigure}
	\caption{Robot models}
\end{figure}

The kinematics of the robot is given by :

We assume that the grip is perfect between the road and the robot wheels, the robot wheels roll without sliding. Under this condition, $V_R$ and $V_L$, the speed of the wheel in the ground reference is given by :

$$\displaystyle V_R = r \times \omega_R \text{     and     } V_L = r \times \omega_L$$

So that, $V$ the velocity of the robot is the sum of both speed divided by two. Decomposition of the velocities in the ground reference $(O, x, y)$, we have :


\begin{align*}
	\displaystyle \dot{x} &= \frac{r}{2} \left(\omega_R + \omega_L\right) \cos \theta\\
	\displaystyle \dot{y} &= \frac{r}{2} \left(\omega_R + \omega_L\right) \sin \theta
\end{align*}


Moreover, we note $\varphi$ the angular velocity of the robot in the robot reference isolating one wheel.


$$\displaystyle \varphi_R = \frac{r}{d} \times \omega_R \text{     and     } \varphi_L =  \frac{r}{d} \times \omega_L$$

In this case, the difference diveded by two of the two gives the angular speed of the robot.

$$\displaystyle \dot{\theta} = \frac{1}{2} \left(\varphi_R - \varphi_L\right)$$

The difference is taken as is to ensure that the angular speed is positive when the angle increases in the anticlockwise direction.

$$\displaystyle \dot{\theta} = \frac{r}{2d} \left(\omega_R - \omega_L\right)$$


\subsubsection{Sensors}

The robot is equiped with 2 sensors by default, an accelerometer that calculates acceleration in x and y axis and the rotation along z axis. The second sensor is a LiDAR sensor.

\vspace{1em}

A LiDAR stand for Light Detection And Ranging is a sensor that emits laser beams and measures the time it takes for the beams to return after hitting an object. This time-of-flight measurement is used to calculate the distance between the sensor and the object. LiDAR is commonly used in robotics for exploration applications. The operation of a LiDAR sensor is illustrated in \autoref{fig:lidar_operation}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{IMAGES/part5/lidar_operation_scheme.png}
	\caption{LiDAR operating diagram}
	\label{fig:lidar_operation}
\end{figure}

In our case, we work in the air, which is why we chose LiDAR. For underwater exploration applications, sonar is used and works on the same principle. Instead of emitting laser beams, sonar emits sound waves. 

\vspace{0.5em}

For estimating inertial parameters, more advanced sensors such as Doppler Velocity Logs (DVL) can be used. They work by emitting sound waves and calculating the Doppler effect to estimate the position and rotation of the robot.

\vspace{1em}

Simulating the LiDAR was challenging in terms of performance. Initially, I attempted to simulate it by shooting rays and computing collisions with any walls in the map. However, this approach severely impacted the simulator's performance. To improve efficiency, I divided the map into multiple cells, storing walls that intersect each cell. By only checking the cells that the LiDAR beam passes through, rather than the entire map, performance was significantly enhanced.

\vspace{1em}

To determine the indices that the LiDAR beam crosses, I implemented a function similar to the Bresenham's line algorithm, which is used to draw lines on a screen. This task was also complex because it was crucial not to miss any cells that the beam passes through. I used a modified version of Bresenham's line algorithm that expands from one cell in all directions. This method, illustrated in \autoref{fig:draw_line}, ensures that all relevant cells are checked, saving a considerable amount of computational time for other tasks.\cite{bresenham_wiki_2025}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{IMAGES/part5/draw_line.png}
	\caption{Drawing a line using Bresenham's algorithm}
	\label{fig:draw_line}
\end{figure}

\subsubsection{Mapping}

Simultaneous Localization and Mapping (SLAM) is a key problem in robotics, enabling a robot to determine its position while simultaneously building a map of its environment. Various techniques have been developed to tackle this challenge. For instance, EKF-SLAM uses a Kalman filter to estimate both the localization and the map but faces scalability limitations in large environments. FastSLAM, which relies on a particle filter, enhances scalability by handling world features through multiple hypotheses.  

\vspace{1em}

Graph-based approaches, such as Graph-SLAM, are effective for large-scale optimization problems but require complex data management. Visual SLAM (V-SLAM) leverages cameras to estimate localization and construct maps, whereas LiDAR-based SLAM relies on laser sensors for precise depth measurements, making it particularly useful for outdoor environments. Dynamic SLAM variants manage environments with moving objects by excluding them from map updates.\cite{ding_2024}

\vspace{1em}

In this work, I used LiDAR-based SLAM. The robot scans its environment using a LiDAR sensor and builds a map based on the collected data. The map is then used to localize the robot within the environment.

\vspace{1em}

The localization process involves determining the collision points where the LiDAR beams intersect with walls. At each collision point, a circle is drawn with a radius equal to the measured LiDAR distance. The robot's position is then estimated by calculating the intersection of multiple such circles. 
The method is visualized in \autoref{fig:lidar_loca}. Here, the central red dot represents the intersection of the circles, the red beams are the LiDAR beams, and the green dotted line indicates the maximum range of the LiDAR.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{IMAGES/part5/lidar_correct_pos.png}
	\caption{Correct robot position using LiDAR data diagram}
	\label{fig:lidar_loca}
\end{figure}


For mapping, I use a feature map, referred to as the live grid map in this section. Each cell in the map can have one over multiple states. When the LiDAR detects a collision, the corresponding cell in the map is assigned an integer value between 100 and 200. The closer the value is to 200, the more certain it is that the cell represents a wall. When the robot passes through a cell, its value is set to 19. If the cell is free, its value is set to 20. A value of 0 indicates that the state of the cell is unknown.

\vspace{1em}

Since the robot operates alongside other robots, the map must account for the presence of multiple dynamic objects. To achieve this, a twin of the live grid map is created, which tracks the number of times a LiDAR beam collides with an object in each cell. Each time the LiDAR detects a collision, 3 is added to the cell in the occurrence map. Conversely, each time the LiDAR beam passes through a cell without detecting a collision, the occurrence value of that cell is decreased by 1. Any cell with an occurrence value above 0 is considered to be occupied by a wall. The higher the occurrence value, the more likely the cell is occupied.

\vspace{1em}

The asymmetry between adding and subtracting values in the occurrence map makes the map more reliable. If the values were symmetric (i.e., adding 1 for a collision and subtracting 1 for no collision), the live grid map would be less secure and more sensitive to changes. For example, in \autoref{fig:lidar_occurence_map}, if the values were symmetric, the green cell would be considered free of obstacles. However, with the current method, the green cell would have a value of $3-1-1=1$, indicating that it is occupied.

\vspace{1em}

This way, if the occurrence value is high, we can be certain that there is a non-moving obstacle in that area. If the occurrence value is low, the cell is less likely to be occupied.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\linewidth]{IMAGES/part5/lidar_occurance_map.png}
	\caption{Importance of asymetry in adding and subtracting in the occurence map}
	\label{fig:lidar_occurence_map}
\end{figure}

\autoref{fig:map_updates} illustrates the map updates over time with three robots. The purple squares represent occupied cells in the live grid map (dark purple = 100, light purple = 200). Green areas indicate free spaces, gray areas are unknown, black lines outline the map, and the small red dots represent the robots:

\begin{figure}[H]
	\centering
	\begin{subfigure}{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{IMAGES/part5/lgm_t1.png}
		\caption{Map update at time $t_1$}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{IMAGES/part5/lgm_t2.png}
		\caption{Map update at time $t_2$}
	\end{subfigure}
	\vfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{IMAGES/part5/lgm_t3.png}
		\caption{Map update at time $t_3$}
	\end{subfigure}
	\caption{Map updates over time}
	\label{fig:map_updates}
\end{figure}






\end{document}